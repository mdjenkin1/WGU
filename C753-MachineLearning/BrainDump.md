# Brain Dump and Development Documentation

To complete this project I'll need to answer a series of questions.  

* What is the goal?
* What features were used and why?
* Which algorithms were tried?
* How was the algorithm tuned?
* How was the analysis validated?
* Generate and evaluate metrics.

In my experience, each company has their own vocabulary. Each group within that company has their own dialect of that vocabulary. It would follow the clique committing fraud would have their own dialect. The goal of our machine learner is to identify this dialect. This dialect can then be used to identify potential people of interest.  

The dataset features targeted by this learning goal would be the bodies of the emails. Without an external corpus, it would be difficult to determine a company wide common vocabulary. Such a determination would only be useful if we needed to identify an Enron employee from a non-employee. We already know everyone in our dataset is an employee. Instead, finding separate dialects within the emails will be our focus.  

We're seeking to split the corpus into two dialects. One used by POI and the other used by non-POI. To process the email data, I would first split them into two groups. One for POI and the other for non-POI. The POI group I would split into training and validation sets. The non-POI group would be split into three groups. One for training, a second for validation and a third for identification.  

With the data set split into training, validation, and identification sets, it's time to determine which algorithm to use to train our learner. One option is to use the TextBlob module for each email sender. For each sender, create a wordblob comprised of all emails they composed. Remove the stop words and generate a frequency count for each sender's word blob. Using the average frequency values for POI, we can use Naive Bayes to generate a probability for email matches the vocabularies of POI and test against the unknown POI dataset. Further direction is contingent on the outcome of this test.  

For determining dialect, it may be necessary to parse emails based on party of origin.  
Duplication of email is a concern for dialect determination.  

"""{Python}
    getPersonsEmails.py: generate pickle dumps of a of person's emails
    Loosely based on the vectorize_text.py and parse_out_email_text.py scripts of the udacity text_learning project
    
    parse_out_email_text.py returns a single string of space delimited stemmed words.
        It splits the email based on the line "X-FileName:"
        Everything before this line is header/metadata (content[0])
        Everything after that line is email body (content[1])
        content[1] is stripped of punctuation and the words are stemmed. This string is what is returned
        
    vectorize_text.py 
        This script generates two lists
            The list word_data contains the parsed data generated by parse_out_email_text. One entry per email.
            The list from_data contains a number representing who sent that email.
            These two lists are serialized and dumped to pickle files
        This script only seems usable as an example of how to access and read the email files.

    After inspecting some emails it might be worth inspecting the metadata. In some cases it holds more information than the body.

    Validation of 'X-FileName:' as a 
        C:\Users\Michael\Documents\WGU\WGU-Projects\C753-MachineLearning\maildir [master ≡ +1 ~0 -0 !]
        λ  gci -recur | ?{$_.PSIsContainer -eq $False} | measure-object
        Count    : 517401
        C:\Users\Michael\Documents\WGU\WGU-Projects\C753-MachineLearning\maildir [master ≡ +1 ~0 -0 !]
        λ  gci -recur |? {$_.PSIsContainer -eq $False} | sls -pattern 'X-FileName:' |measure-object
        Count    : 517401
"""
