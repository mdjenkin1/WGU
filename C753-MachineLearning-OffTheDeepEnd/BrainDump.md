# Brain Dump and Development Documentation

To complete this project I'll need to answer a series of questions.  

* What is the goal?
* What features were used and why?
* Which algorithms were tried?
* How was the algorithm tuned?
* How was the analysis validated?
* Generate and evaluate metrics.

In my experience, each company has their own vocabulary. Each group within that company has their own dialect of that vocabulary. It would follow the clique committing fraud would have their own dialect. The goal of our machine learner is to identify this dialect. This dialect can then be used to identify potential people of interest.  

The dataset features targeted by this learning goal would be the bodies of the emails. Without an external corpus, it would be difficult to determine a company wide common vocabulary. Such a determination would only be useful if we needed to identify an Enron employee from a non-employee. We already know everyone in our dataset is an employee. Instead, finding separate dialects within the emails will be our focus.  

We're seeking to split the corpus into two dialects. One used by POI and the other used by non-POI. To process the email data, I would first split them into two groups. One for POI and the other for non-POI. The POI group I would split into training and validation sets. The non-POI group would be split into three groups. One for training, a second for validation and a third for identification.  

With the data set split into training, validation, and identification sets, it's time to determine which algorithm to use to train our learner. One option is to use the TextBlob module for each email sender. For each sender, create a wordblob comprised of all emails they composed. Remove the stop words and generate a frequency count for each sender's word blob. Using the average frequency values for POI, we can use Naive Bayes to generate a probability for email matches the vocabularies of POI and test against the unknown POI dataset. Further direction is contingent on the outcome of this test.  

For determining dialect, it may be necessary to parse emails based on party of origin.  
Duplication of email is a concern for dialect determination.  

```{Python}
"""
    getPersonsEmails.py: generate pickle dumps of a of person's emails
    Loosely based on the vectorize_text.py and parse_out_email_text.py scripts of the udacity text_learning project

    parse_out_email_text.py returns a single string of space delimited stemmed words.
        It splits the email based on the line "X-FileName:"
        Everything before this line is header/metadata (content[0])
        Everything after that line is email body (content[1])
        content[1] is stripped of punctuation and the words are stemmed. This string is what is returned

    vectorize_text.py
        This script generates two lists
            The list word_data contains the parsed data generated by parse_out_email_text. One entry per email.
            The list from_data contains a number representing who sent that email.
            These two lists are serialized and dumped to pickle files
        This script only seems usable as an example of how to access and read the email files.

    After inspecting some emails it might be worth inspecting the metadata. In some cases it holds more information than the body.

    Validation of 'X-FileName:' as a 
        C:\Users\Michael\Documents\WGU\WGU-Projects\C753-MachineLearning\maildir [master ≡ +1 ~0 -0 !]
        λ  gci -recur | ?{$_.PSIsContainer -eq $False} | measure-object
        Count    : 517401
        C:\Users\Michael\Documents\WGU\WGU-Projects\C753-MachineLearning\maildir [master ≡ +1 ~0 -0 !]
        λ  gci -recur |? {$_.PSIsContainer -eq $False} | sls -pattern 'X-FileName:' |measure-object
        Count    : 517401
"""
```

The smallest identifiable clique consists of an individual.
An individual's clique is defined by the vocabulary of those around the individual
Shared clique vocabulary between individuals identifies members of the same clique
Expectation has POI within the same clique.

For each person  <!-- TOC -->autoauto- [Brain Dump and Development Documentation](#brain-dump-and-development-documentation)autoauto<!-- /TOC -->

1. Generate a corpus from each person's email dump
    * dump_emails.py: Master script for email processing. Generates a dictionary of persons and a corpus of their emails.
1. Preprocess each corpus
    * parse_out_email_text.py: performs the following preprocessing of each email
        * Strip email header
        * Tokenize the email into words
        * Stem the words
    * Lemmatization was considered over stemming. Stemming was selected as word use is of more interest than meaning. It may be of interest to instead ignore this step and process raw words.
    * Weigh: Word weights performed by TfIdf.
        * TfIdf was selected for weights as it values frequent and distinctive words.
        * Frequent words are considered the foundation of a clique's vocabulary
        * Distinctive words are markers of dialect. Each clique will have its own dialext of the company's vocabulary.
    * Normalize - is this necessary?
1. Identify features from the corpus to describe a clique
    * Principal Component Analysis
        * Word by word
            * A country of people will use the same word base. Only when you get to various regions would you expect sparse words.
        * ~~PCA is not a replacement for feature selection.~~ PCA is a technique for feature extraction [https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)
        * Dimensionality Reduction
    * Feature Selection
        * There's a large number of variant features that need to be balanced against a large number of homogenous features. A technique needs to be found for reducing the number of homogenous features and the number of variant features. Based on assumptions of what model this data should fit there is extra concern of overfitting.
    * Data Shaping
        * From each persons corpus we have
            * A collection of scipy.sparse.csr_matrix
                * Each 
            * A tfidfvectorizer
                * tfidfvec.get_feature_names() returns (feature : index)

```{python}
vec = vocab[dude]['tfidfvec']
vec
len(vec.get_feature_names())


doc = 0
feature_index = tmx[doc,:].nonzero()[1]
tfidf_scores = zip(feature_index, [tmx[doc, x] for x in feature_index])
feature_names = vec.get_feature_names()
for w,s in [(feature_names[i], s) for (i, s) in tfidf_scores]:
    print w,s
```


1. What other methods are there
1. Group individuals with similar cliques
    * K-means clustering
1. Determine accuracy by comparing identified cliques to the list of known POI
    * How many known POI are grouped with the 

Count of word inclusion in corpus for subset.

119953 words in total
90178 words appear in 1 corpora
13470 words appear in 2 corpora
5368 words appear in 3 corpora
3474 words appear in 4 corpora
2264 words appear in 5 corpora
1824 words appear in 6 corpora
1591 words appear in 7 corpora
1784 words appear in 8 corpora


The words used most by an individual 
Reducing the

Latent Semantic Analysis
